{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste notebook, vamos explorar um pouco a Expansão de Consultas na Recuperação de Informação. A Expansão de consultas é uma ferramenta que nos auxilia a melhorar a consulta passada pelo usuário, através da análise da relação entre as palavras da consulta e as palavras que vem depois delas nos documentos. Assim, podemos inferir que a palavra que mais aparece depois de outra tem uma forte ligação com ela.\n",
    "\n",
    "Primeiro, vamos montar a matriz de coocorrência. As linhas e colunas dessa matriz são as palavras do nosso dicionário, e a relação matriz[palavra1][palavra2] nos da a quantidade de vezes que a palavra 2 apareceu imediatamente depois da palavra1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documentos = pd.read_csv(\"../../modelo_vetorial/estadao_noticias_eleicao.csv\")\n",
    "documentos = documentos.replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content = documentos.titulo + \" \" + documentos.subTitulo + \" \" + documentos.conteudo\n",
    "content = content.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def co_occurrence_matrix(corpus):\n",
    "    vocab = set(corpus)\n",
    "    vocab = list(vocab)\n",
    "    n = len(vocab)\n",
    "   \n",
    "    vocab_to_index = {word:i for i, word in enumerate(vocab)}\n",
    "    \n",
    "    bi_grams = list(bigrams(corpus))\n",
    "\n",
    "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
    "\n",
    "    I=list()\n",
    "    J=list()\n",
    "    V=list()\n",
    "    \n",
    "    for bigram in bigram_freq:\n",
    "        current = bigram[0][1]\n",
    "        previous = bigram[0][0]\n",
    "        count = bigram[1]\n",
    "\n",
    "        I.append(vocab_to_index[previous])\n",
    "        J.append(vocab_to_index[current])\n",
    "        V.append(count)\n",
    "        \n",
    "    co_occurrence_matrix = sparse.coo_matrix((V,(I,J)), shape=(n,n))\n",
    "\n",
    "    return co_occurrence_matrix, vocab_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removendo pontuação\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens_lists = content.apply(lambda text: tokenizer.tokenize(text.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo stopwords\n",
    "stopword_ = stopwords.words('portuguese')\n",
    "filtered_tokens = tokens_lists.apply(lambda tokens: [token for token in tokens if token not in stopword_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = [token for tokens_list in filtered_tokens for token in tokens_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix, vocab = co_occurrence_matrix(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coocorrenciaMatrix = matrix.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabIds = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função abaixo expande o termo passado como parâmetro, retornando os termos que aparecem com mais frequência depois dele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Função que expande o termo passado como parâmetro para os termos mais\n",
    "próximos a ele.\n",
    "\n",
    "@param termo - termo a ser expandido\n",
    "@param numeroDeExpansoes quantidade de termos a serem expandidos\n",
    "\n",
    "@returns Array expansão dos termos\n",
    "'''\n",
    "def expande_termo(termo, numeroDeExpansoes):\n",
    "    linha = coocorrenciaMatrix[vocab[termo]].toarray()\n",
    "    termosExpandidos = np.argpartition(linha[0], -numeroDeExpansoes)[-numeroDeExpansoes:]\n",
    "    expansao = []\n",
    "    for idTermo in termosExpandidos:\n",
    "        expansao.append(vocabIds[idTermo])\n",
    "    return expansao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que temos a matriz de coocorrência dos termos e a função que realiza a expansão dos termos, vamos montar o nosso índice invertido com os documentos apresentados. A implementação será a mesma utilizada nos labs passados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def geraIndice(documentos):\n",
    "    indice_invertido = dict()\n",
    "    for i, row in documentos.iterrows():\n",
    "        \n",
    "        titulo = (word.lower() for word in (nltk.word_tokenize(row['titulo'])))\n",
    "        subtitulo = (word.lower() for word in (nltk.word_tokenize(row['subTitulo'])))\n",
    "        conteudo = (word.lower() for word in (nltk.word_tokenize(row['conteudo'])))\n",
    "        palavras_divididas = list(titulo) + list(subtitulo) + list(conteudo)\n",
    "        for palavra in palavras_divididas:\n",
    "            if palavra not in indice_invertido:\n",
    "                indice_invertido[palavra] = set([row['idNoticia']])\n",
    "            else:\n",
    "                indice_invertido[palavra].add(row['idNoticia'])\n",
    "    \n",
    "    return indice_invertido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indice_invertido = geraIndice(documentos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este lab, vamos intanciar o modelo binário. As consultas realizadas serão a disjunção dos termos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def consultaOR(termos):\n",
    "    indice = []\n",
    "    for termo in termos:\n",
    "        indice.append(indice_invertido[termo])\n",
    "    return list(set.intersection(*indice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def consultaExpandida(termos):\n",
    "    termosExpansao = []\n",
    "    for termo in termos:\n",
    "        termosExpansao = termosExpansao + expande_termo(termo, 3)\n",
    "    return termos + termosExpansao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que temos uma função que expande os termos da consulta, vamos escolher três termos e ver como fica a consulta expandida pra eles. Os termos serão PT, Petrobrás e Lula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PT: \n",
      "['pmdb', 'governo', 'psdb']\n",
      "Petrobrás\n",
      "['graça', 'é', 'paulo']\n",
      "Lula\n",
      "['disse', 'dilma', 'silva']\n"
     ]
    }
   ],
   "source": [
    "print(\"PT: \")\n",
    "print(expande_termo(\"pt\", 3))\n",
    "print(\"Petrobrás\")\n",
    "print(expande_termo(\"petrobrás\", 3))\n",
    "print(\"Lula\") \n",
    "print(expande_termo(\"lula\", 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, os termos expandidos são extremamente relacionados com o termo original. PMDB, PSDB são partidos políticos assim como o PT. O nosso filtro não funcionou muito bem com as stopwords, tendo trazido o \"é\" para Petrobrás. Já Lula também teve termos muito relacionados, como Dilma, a presidenta que o sucedeu, Silva, seu sobrenome. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora realizar uma consulta com os termos expandidos e sem para ver como fica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consultaOriginal = [\"presidente\", \"lula\"]\n",
    "documentosSemExpansao = consultaOR(consultaOriginal)\n",
    "documentosComExpansao = consultaOR(consultaExpandida(consultaOriginal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A consulta escolhida foi \"Presidente Lula\". Veremos agora quais os documentos retornados por cada consulta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sem expansão\n",
      "[1, 2, 3, 8197, 8198]\n",
      "\n",
      "\n",
      "Com expansão\n",
      "[6148, 4619, 6156, 4112, 4625]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sem expansão\")\n",
    "print(documentosSemExpansao[:5])\n",
    "print(\"\\n\")\n",
    "print(\"Com expansão\")\n",
    "print(documentosComExpansao[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, nenhum documento retornado na consulta original está na consulta expandida. Isso acontece por que a quantidade de documentos que é retornada na consulta expandida é bem maior que na consulta original. Isso deve-se ao fato que a consulta OR é a união dos conjuntos dos documentos que contém algum dos termos.\n",
    "\n",
    "A expansão é interessante por trazer mais documentos que provavelmente estão relacionados aos termos originalmente requisitados. Se utilizássemos outro modelo, como o BM25, por exemplo, para rankear os documentos, provavelmente obteriamos resultados melhor utilizando a expansão.\n",
    "\n",
    "De modo geral, a expansão ajuda a melhorar o recall por trazer termos que estão relacionados a consulta original,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
